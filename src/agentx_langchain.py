# src/agentx_langchain.py
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# LLM
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    api_key=os.getenv("OPENAI_API_KEY"),
)

# Prompt com placeholder para histÃ³rico
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "VocÃª Ã© o AgentX. Seja Ãºtil, conciso e educado."),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}"),
    ]
)

# Cadeia bÃ¡sica: prompt -> LLM
chain = prompt | llm

# Store de histÃ³ricos por sessÃ£o
store: dict[str, ChatMessageHistory] = {}

def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# Adapta a cadeia para manter histÃ³rico
chat_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)

if __name__ == "__main__":
    print("ğŸ¤– AgentX (LangChain) com memÃ³ria de curto prazo. Digite 'sair' para encerrar.")
    session_id = "cli"  # poderia ser um ID do usuÃ¡rio
    while True:
        user = input("> ").strip()
        if user.lower() in {"sair", "exit", "quit"}:
            print("AtÃ© mais! ğŸ‘‹")
            break
        resp = chat_with_history.invoke(
            {"input": user},
            config={"configurable": {"session_id": session_id}},
        )
        # resp Ã© um AIMessage
        print(resp.content)
