# ğŸ“˜ MÃ³dulo 4 â€” LangChain e MemÃ³ria de Agentes

## ğŸ¯ Objetivo

Dar **memÃ³ria** ao AgentX para que ele use contexto recente (curto prazo) e **lembre** informaÃ§Ãµes entre sessÃµes (memÃ³ria persistente com FAISS). TambÃ©m padronizamos a execuÃ§Ã£o via **Docker Compose** e `.env`.

---

## ğŸ“š Conceitosâ€‘chave

* **RunnableWithMessageHistory**: forma moderna do LangChain para manter histÃ³rico de mensagens sem classes deprecadas.
* **MemÃ³ria de curto prazo**: histÃ³rico sÃ³ da sessÃ£o atual (em memÃ³ria).
* **MemÃ³ria persistente**: usa **FAISS** para salvar e consultar fatos passados.
* **Retriever consciente do histÃ³rico**: reformula perguntas usando o histÃ³rico para recuperar documentos relevantes.
* **.env + Docker Compose**: variÃ¡veis de ambiente seguras e execuÃ§Ã£o simplificada.

---

## ğŸ›  Passos prÃ¡ticos (80/20)

1. **Instalamos dependÃªncias** (no `requirements.txt`):

   ```txt
   langchain
   langchain-openai
   langchain-community
   faiss-cpu
   ```
2. **Criamos/atualizamos arquivos**:

   * `src/agentx_langchain.py` â†’ memÃ³ria de **curto prazo** (sem warnings).
   * `src/agentx_langchain_faiss.py` â†’ memÃ³ria **persistente** com FAISS.
   * `.env.example` + `.env` â†’ chaves de API.
   * `docker-compose.yml` â†’ serviÃ§os prontos para cada mÃ³dulo.
3. **Rodamos com Compose**:

   ```bash
   docker compose build
   docker compose run --rm agentx-langchain           # curto prazo
   docker compose run --rm agentx-langchain-faiss     # persistente
   ```

---

## ğŸ’¡ Miniâ€‘projeto aplicado

* **AgentX com histÃ³rico de sessÃ£o** (curto prazo) para conversas mais coerentes.
* **AgentX com FAISS** (persistente) para lembrar fatos entre execuÃ§Ãµes.

---

## ğŸ“„ CÃ³digo adicionado/alterado

### `src/agentx_langchain.py` â€” MemÃ³ria de curto prazo

```python
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# LLM
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    api_key=os.getenv("OPENAI_API_KEY"),
)

# Prompt com placeholder para histÃ³rico
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "VocÃª Ã© o AgentX. Seja Ãºtil, conciso e educado."),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}"),
    ]
)

# Cadeia bÃ¡sica: prompt -> LLM
chain = prompt | llm

# Store de histÃ³ricos por sessÃ£o
store: dict[str, ChatMessageHistory] = {}

def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# Adapta a cadeia para manter histÃ³rico
chat_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)

if __name__ == "__main__":
    print("ğŸ¤– AgentX (LangChain) com memÃ³ria de curto prazo. Digite 'sair' para encerrar.")
    session_id = "cli"  # poderia ser um ID do usuÃ¡rio
    while True:
        user = input("> ").strip()
        if user.lower() in {"sair", "exit", "quit"}:
            print("AtÃ© mais! ğŸ‘‹")
            break
        resp = chat_with_history.invoke(
            {"input": user},
            config={"configurable": {"session_id": session_id}},
        )
        print(resp.content)
```

### `src/agentx_langchain_faiss.py` â€” MemÃ³ria persistente (FAISS)

```python
import os
from pathlib import Path

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

DB_PATH = "data/faiss_memory"
os.makedirs("data", exist_ok=True)

# LLM e Embeddings
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.5,
    api_key=os.getenv("OPENAI_API_KEY"),
)
embeddings = OpenAIEmbeddings(api_key=os.getenv("OPENAI_API_KEY"))

# Carrega ou cria FAISS
if Path(DB_PATH).exists():
    vectorstore = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)
else:
    vectorstore = FAISS.from_texts(["MemÃ³ria inicial do AgentX."], embeddings)
    vectorstore.save_local(DB_PATH)

retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# 1) Retriever ciente do histÃ³rico (reformula a pergunta conforme o contexto)
rephrase_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Reescreva a pergunta do usuÃ¡rio considerando o histÃ³rico da conversa."),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
history_aware_retriever = create_history_aware_retriever(
    llm=llm, retriever=retriever, prompt=rephrase_prompt
)

# 2) Cadeia que insere documentos + responde
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "Use o contexto para responder. Se nÃ£o souber, diga que nÃ£o sabe.\n\nContexto:\n{context}"),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
doc_chain = create_stuff_documents_chain(llm, qa_prompt)

# 3) Retrieval chain completa
retrieval_chain = create_retrieval_chain(history_aware_retriever, doc_chain)

# HistÃ³rico por sessÃ£o
store: dict[str, ChatMessageHistory] = {}
def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# 4) Encapsula com histÃ³rico (a chave do histÃ³rico aqui Ã© 'chat_history')
qa_with_history = RunnableWithMessageHistory(
    retrieval_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

if __name__ == "__main__":
    print("ğŸ¤– AgentX (LangChain + FAISS) com memÃ³ria persistente. Digite 'sair' para encerrar.")
    session_id = "cli"
    while True:
        user = input("> ").strip()
        if user.lower() in {"sair", "exit", "quit"}:
            print("AtÃ© mais! ğŸ‘‹")
            break

        result = qa_with_history.invoke(
            {"input": user},
            config={"configurable": {"session_id": session_id}},
        )
        answer = result["answer"] if isinstance(result, dict) else str(result)
        print(answer)

        # Salva o par Q/A na base vetorial (persistÃªncia)
        vectorstore.add_texts([f"Pergunta: {user}\nResposta: {answer}"])
        vectorstore.save_local(DB_PATH)
```

### `.env.example`

```dotenv
# Copie para .env e preencha com suas chaves reais
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
SERPER_API_KEY=serper_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### `docker-compose.yml`

```yaml
version: "3.9"

services:
  agentx:
    build: .
    env_file:
      - .env
    tty: true
    stdin_open: true
    command: ["python", "src/main.py"]

  agentx-agno:
    build: .
    env_file:
      - .env
    tty: true
    stdin_open: true
    command: ["python", "src/agentx_agno.py"]

  agentx-langchain:
    build: .
    env_file:
      - .env
    tty: true
    stdin_open: true
    command: ["python", "src/agentx_langchain.py"]

  agentx-langchain-faiss:
    build: .
    env_file:
      - .env
    tty: true
    stdin_open: true
    volumes:
      - ./data:/app/data
    command: ["python", "src/agentx_langchain_faiss.py"]

  agentx-crewai:
    build: .
    env_file:
      - .env
    tty: true
    stdin_open: true
    command: ["python", "src/agentx_crewai.py"]
```

---

## ğŸ§ª Como testar (Compose)

```bash
docker compose build

docker compose run --rm agentx-langchain           # curto prazo
# Exemplo:
# > Oi, eu sou a Maria
# > Qual Ã© o meu nome?

/docker compose run --rm agentx-langchain-faiss     # persistente
# Diga algo para persistir, saia e rode de novo: o agente deve lembrar
```

---

## ğŸ“ ExercÃ­cios de fixaÃ§Ã£o

1. Guardar **apenas fatos importantes** no FAISS (ex.: â€œmeu nome Ã©â€¦â€, â€œmeu e-mail Ã©â€¦â€).
2. Adicionar um comando `exportar_memoria` que compacte a pasta `data/`.
3. Trocar o FAISS por **Chroma** e comparar.

---

## ğŸ”„ RevisÃ£o rÃ¡pida

* Removemos depreciaÃ§Ãµes do LangChain.
* Implementamos memÃ³ria de **curto prazo** e **persistente**.
* Padronizamos execuÃ§Ã£o com **`.env` + Docker Compose**.

---

## ğŸš€ PrÃ³ximo passo

Integrar ferramentas ao pipeline do LangChain (web, arquivos, banco de dados) e conectar a memÃ³ria com aÃ§Ãµes reais â€” preparando o MÃ³dulo 5 (IntegraÃ§Ãµes e AutomaÃ§Ã£o).
